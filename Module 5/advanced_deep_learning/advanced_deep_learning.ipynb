{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Topics\n",
    "\n",
    "### * Word2Vec\n",
    "### * Sequence Models\n",
    "### * Autoencoders\n",
    "### * Variational Autoencoders\n",
    "### * GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "* What even is meaning???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence = 'the dog had puppies and the cat had kittens'\n",
    "bow = CountVectorizer().fit([sentence])\n",
    "sentence_transformed = bow.transform([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/dog_cat.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about TF-IDF??**\n",
    "\n",
    "TF-IDF is an attempt to get some more meaning, but it is unable to do anything more than determine which words are important. It does not actually give us any insight into the meaning of words in relation to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we were able to create a model that would be able to determine how words related to one another??\n",
    "\n",
    "<img src=\"./resources/vec-relationships.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening under the hood\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "* Every word will have a dense vector\n",
    "Skip-Grams\n",
    "* Predict context words given a certain target (position independent)\n",
    "* For each word, what is the probability of the other words, with a radius *m*\n",
    "* Use softmax to obtain the probability of word *c* to obtain other words *o*\n",
    "\n",
    "\n",
    "$ p(context|word_{t})= ....$  \n",
    "We are minimizing a loss function $ J = 1-p(w_{-t}|w_{t}) $\n",
    "\n",
    "\n",
    "<img src = \"./resources/skip_grams.png\">\n",
    "\n",
    "<img src=\"./resources/softmax-nplm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How we can best make use of it?\n",
    "\n",
    "Use transfer learning of GloVe!! https://nlp.stanford.edu/projects/glove/  \n",
    "\n",
    "**Glo**bal **Ve**ctors of word representation  \n",
    "\n",
    "cool visual example of words in space: https://projector.tensorflow.org/\n",
    "\n",
    "analogies with word2vec: http://bionlp-www.utu.fi/wv_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### what Word2Vec code looks like\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models (Recurrent Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "* Machine Translation\n",
    "* Time series predictions\n",
    "* Speech recognition\n",
    "* Music composition (https://soundcloud.com/rapping_neural_network/networks-with-attitude)\n",
    "* Rhythym Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening under the hood\n",
    "BackPropagation Through Time: http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf\n",
    "<img src = \"./resources/Recurrent_neural_network_unfold.svg\">\n",
    "<img src = \"./resources/unfolded.png\">\n",
    "\n",
    "##### Wow this is going to be a lot of different layers, especially if we have numerous recurrent nodes. Can you foresee any issues with this?\n",
    "#### LSTM\n",
    "\n",
    "<img src =\"./resources/Long_Short-Term_Memory.svg\">\n",
    "\n",
    "* Input Gate: Determines how much of the cell state that was passed along should be kept\n",
    "* Forget Gate: Which determines how much of the current state should be forgotten\n",
    "* Output Gate: Which determines how much of the current state should be exposed to the next layers of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM in code\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(20000, 128))\n",
    "lstm_model.add(LSTM(50, return_sequences=True))\n",
    "lstm_model.add(GlobalMaxPool1D())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(50, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU (Gated Recurrent Unit)\n",
    "\n",
    "<img src = \"./resources/Gated_Recurrent_Unit.svg\">\n",
    "\n",
    "* Reset Gate: Determines what should be removed from the cell's internal state before passing itself along to the next time step.\n",
    "* Update Gate: Determines how much of the state from the previous time step should be used in the current time tep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model in code\n",
    "\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(20000, 128))\n",
    "gru_model.add(GRU(50, return_sequences=True))\n",
    "gru_model.add(GlobalMaxPool1D())\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(50, activation='relu'))\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which is better????????\n",
    "* it depends on the context!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Sequence Models\n",
    "\n",
    "Work extremely well for NLP tasks. Within each recurrent layer, half of the neurons will move from forward to in the sequence to the end of the sequence. The other half of the neurons will move in reverse! The model will then use a formula to combine the results of both the outputs from the forward-in-time and backward-in-time neurons at each time step. We can choose this formula as one of the hyperparameters of keras class:  \n",
    "\n",
    "Bidirectional(merge_mode= )\n",
    "\n",
    "<img src =\"./resources/bidirectional.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                        input_shape=(5, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method of unsupervised (or semi-supervised) learning with a neural network. It is useful for:  \n",
    "* feature extraction and dimensionality reduction\n",
    "* denoising images\n",
    "\n",
    "<img src=\"./resources/enhance.gif\">\n",
    "\n",
    "Three main components:  \n",
    "1. Encoder:  Takes the original input and compresses it into a latent space representation.  \n",
    "2. Code:  The compressed version of the original input.  \n",
    "3. Decoder: Decodes the coded data back to an approximation of the original input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening under the hood\n",
    "\n",
    "<img src =\"./resources/autoencoder.png\">\n",
    "\n",
    "Hyperparameters to tune:\n",
    "* Number of layers\n",
    "* Number of nodes per layer: typically the number of nodes ends up forming an hour-glass shape and is symmetrical\n",
    "* Loss function: mean square error or binary crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDentify input and encoding dimensions\n",
    "input_dim = x_train.shape[1] # input dimension = 784\n",
    "encoding_dim = 32\n",
    "\n",
    "# Calculate the compression factor\n",
    "compression_factor = float(input_dim) / encoding_dim\n",
    "print(\"Compression factor: %s\" % compression_factor)\n",
    "\n",
    "# Build the autoencoder model \n",
    "autoencoder = Sequential()\n",
    "# Encoder Layer\n",
    "autoencoder.add(Dense(encoding_dim, input_shape=(input_dim,), activation='relu'))\n",
    "#Decoder Layer\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "# Show model summary\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoder\n",
    "\n",
    "<img src =\"./resources/denoising_mnist.jpg\">\n",
    "\n",
    "How it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noisy train and test datasets\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "# Clip the datasets to ensure data integrity\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Input dimensions\n",
    "x = Input(name='inputs', shape=input_shape, dtype='float32')\n",
    "    \n",
    "# Encoder model \n",
    "encoder = Dense(32, activation='relu')(x)\n",
    "    \n",
    "# Decoder model \n",
    "decoder = Dense(input_shape[0], activation='sigmoid')(encoder)\n",
    "    \n",
    "# Print network summary\n",
    "DAE = Model(inputs=x, outputs=decoder)\n",
    "DAE.summary()\n",
    "\n",
    "\n",
    "# Compile the model with given parameters\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "DAE.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Fit the data \n",
    "DAE.fit(x_train_noisy, x_train, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        validation_data=(x_test_noisy, x_test))\n",
    "\n",
    "# Predict images\n",
    "decoded_imgs = DAE.predict(x_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional AutoEncoder\n",
    "\n",
    "An autoencoder that works even better for images! Same idea as autoencoder, except the filters are being updated rather than the weights\n",
    "\n",
    "Refresher on Convolutions in CNNs\n",
    "\n",
    "<img src = \"./resources/convolution.gif\">\n",
    "\n",
    "\n",
    "How does this work in the context of ConvAutoEncoders???\n",
    "\n",
    "<img src =\"./resources/conv-autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/faces_transformed.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not standard AutoEncoders?\n",
    "\n",
    "* Autoencoders are work incredibly well for recreating images, but they lack the ability to produce anything other than recreating the original image\n",
    "* The latent space created by standard Auto Encoders is frequently discrete in nature.\n",
    "\n",
    "<img src=\"./resources/clus_encoded.png\">\n",
    "\n",
    "\n",
    "What if we wanted to try to estimate the areas where there was no data? In order to do so, we can sample from continuous distributions.\n",
    "<img src=\"./resources/vae1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs (Generative Adversarial Network)\n",
    "\n",
    "Pits a generator and a discriminator against each other. Frequently, are components within GANs.\n",
    "\n",
    "<img src=\"./resources/GANs.png\">\n",
    "\n",
    "#### Deep Fakes\n",
    "\n",
    "<img src=\"./resources/deep_fake.webp\">\n",
    "\n",
    "#### Style Transfer\n",
    "<img src=\"./resources/style_transfer.jpg\">\n",
    "\n",
    "Cool applications: https://affinelayer.com/pixsrv/  \n",
    "Learn more about GANs: https://www.youtube.com/watch?v=9JpdAg6uMXs&t=1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So if you really like deep learning stuff and you don't want to get a PhD......\n",
    "\n",
    "Industry has a need for people involved in Aritifical Intelligence without spending 5 years getting a PhD. Check out these new residency programs/fellowships:\n",
    "\n",
    "* https://github.com/dangkhoasdc/awesome-ai-residency\n",
    "\n",
    "Quote from the Microsoft AI Residency:\n",
    "\n",
    "\"We are searching for a diverse range of researchers, engineers, and applied scientists with **unique perspectives, including candidates who may not have a traditional background in AI, but who are passionate about working on AI technologies to solve real-world challenges.**\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More resources: https://github.com/terryum/awesome-deep-learning-papers#understanding--generalization--transfer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
